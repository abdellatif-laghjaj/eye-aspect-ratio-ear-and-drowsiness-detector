{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h1 align=\"center\">**Master Big Data et Intelligence Artificielle**</h1>\n",
    "#### <h1 align=\"center\">**Examen Pratique de Traitement d'Images**</h1>\n",
    "##### <h1 align=\"center\">**Session normale - 2023/2024**</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réalisé par : LAGHJAJ ABDELLATIF\n",
    "### Encadré par : Youssef ES-SAADY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import dlib\n",
    "import platform\n",
    "import imutils\n",
    "from imutils import face_utils\n",
    "from scipy.spatial import distance as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction plot_images permet d'afficher plusieurs images dans une grille en utilisant la bibliothèque Matplotlib. Elle prend en entrée une liste d'images, une liste optionnelle de titres pour chaque image, et une taille de figure facultative. La fonction calcule le nombre de lignes et de colonnes nécessaires pour afficher les images de manière compacte dans une grille. Ensuite, elle crée une figure avec des sous-graphiques Matplotlib et itère à travers ces sous-graphiques pour afficher chaque image avec son titre correspondant (si fourni). Enfin, elle ajuste l'espacement entre les sous-graphiques pour une présentation visuelle optimale et affiche la grille d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, titles=None, figsize=(15, 10)):\n",
    "    \"\"\"Display multiple images in a grid.\"\"\"\n",
    "    num_frames = len(images)\n",
    "    rows = int(num_frames ** 0.5) + 1  # Calculate number of rows\n",
    "    cols = (num_frames // rows) + 1    # Calculate number of columns\n",
    "    \n",
    "    if titles is None:\n",
    "        titles = [''] * len(images)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i < len(images):\n",
    "            ax.imshow(images[i])\n",
    "            ax.set_title(titles[i])\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extraction des images à partir d'une vidéo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction extract_frames_at_intervals permet d'extraire des images à partir d'une vidéo à des intervalles réguliers. Elle prend en paramètres le chemin du fichier vidéo (video_path) et un intervalle de temps spécifié en secondes (interval). La fonction ouvre la vidéo, calcule la durée totale de la vidéo et extrait les images à des instants définis par l'intervalle.\n",
    "\n",
    "La vidéo est lue frame par frame, en utilisant la fonction cv2.VideoCapture de la bibliothèque OpenCV. La position de la frame dans la vidéo est ajustée en fonction du temps spécifié par l'intervalle, puis la frame est ajoutée à une liste. Cette liste d'images extraites est ensuite renvoyée par la fonction.\n",
    "\n",
    "La fonction gère également les erreurs potentielles liées à l'ouverture du fichier vidéo et à la lecture des frames, en affichant des messages d'erreur appropriés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract frames from a video pour chaque instant t\n",
    "def extract_frames_at_intervals(video_path, interval = 10):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Unable to open video file.\")\n",
    "        return []\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / cap.get(cv2.CAP_PROP_FPS)  # Duration in seconds\n",
    "\n",
    "    frames = []\n",
    "    for t in range(0, int(duration), interval):\n",
    "        frame_time = t * total_frames / duration\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_time))\n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if ret:\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            print(f\"Error: Unable to read frame at {t} seconds.\")\n",
    "\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conversion des images en niveaux de gris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction convert_to_grayscale prend une liste d'images au format BGR en entrée et retourne une liste d'images converties en niveaux de gris. Pour chaque image de la liste, la fonction utilise la bibliothèque OpenCV pour d'abord convertir l'image de BGR à RGB, puis la convertit de RGB à niveaux de gris. Le résultat est une liste d'images en niveaux de gris correspondantes à celles fournies en entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayscale(frames):\n",
    "    # Convert each BGR image to grayscale\n",
    "    images_gray = [cv2.cvtColor(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), cv2.COLOR_RGB2GRAY) for image in frames]\n",
    "    return images_gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Suppression de bruits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction remove_noise permet de supprimer le bruit d'une ou plusieurs images en utilisant des filtres spécifiés. Elle prend en entrée une liste d'images ou une image unique, un paramètre de méthode ('gaussian' ou 'median') pour choisir le type de filtre, la taille du noyau du filtre, et l'écart-type (pour le filtre gaussien). La fonction retourne une liste d'images sans bruit, où le bruit a été atténué en fonction des paramètres spécifiés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(images, method='gaussian', kernel_size=3, sigma=1.0):\n",
    "    if not isinstance(images, list):\n",
    "        images = [images]\n",
    "\n",
    "    if method == 'gaussian':\n",
    "        return [cv2.GaussianBlur(image, (kernel_size, kernel_size), sigma) for image in images]\n",
    "    elif method == 'median':\n",
    "        return [cv2.medianBlur(image, kernel_size) for image in images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Amélioration de la qualité des images (contraste, luminosité)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction enhance_images améliore la qualité des images en appliquant le contraste adaptatif à l'aide de l'algorithme CLAHE (Contrast Limited Adaptive Histogram Equalization). Elle prend en entrée une liste d'images filtrées, les convertit en niveaux de gris, puis applique le CLAHE individuellement à chaque image en niveaux de gris. Le résultat est une liste d'images améliorées en contraste, prêtes à être affichées correctement en niveaux de gris sur un écran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_images(images_filtered):\n",
    "    # Convertir les images en niveaux de gris\n",
    "    images_gray = [cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in images_filtered]\n",
    "\n",
    "    # Appliquer CLAHE à chaque image en niveaux de gris\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    images_enhanced = [clahe.apply(image) for image in images_gray]\n",
    "\n",
    "    # Convertir les images en niveaux de gris en images BGR pour l'affichage correct\n",
    "    images_bgr = [cv2.cvtColor(image_gray, cv2.COLOR_GRAY2BGR) for image_gray in images_enhanced]\n",
    "\n",
    "    return images_bgr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Détection des contours (utilisant des méthodes telles que Canny ou Sobel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction detect_edges utilise l'opérateur de détection de contours Canny pour détecter les contours dans chaque image améliorée en contraste. Elle prend en entrée une liste d'images améliorées, applique l'algorithme Canny à chaque image individuellement avec des paramètres spécifiés, puis convertit les images résultantes en niveaux de gris en images BGR pour assurer un affichage correct. Le résultat final est une liste d'images affichant les contours détectés, prêtes à être visualisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_edges(images_enhanced):\n",
    "    # Appliquer Canny à chaque image\n",
    "    images_edges = [cv2.Canny(image_enhanced, 100, 200) for image_enhanced in images_enhanced]\n",
    "    \n",
    "    #convertir les images en niveaux de gris en images BGR pour l'affichage correct\n",
    "    images_edges = [cv2.cvtColor(image_edges, cv2.COLOR_GRAY2BGR) for image_edges in images_edges]\n",
    "    return images_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde des images traitées dans un dossier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction save_frames_to_folder permet d'enregistrer chaque image d'une liste de frames dans un dossier spécifié. Elle prend en entrée la liste des frames, le chemin du dossier de destination, et un préfixe optionnel pour le nom de chaque image. Si le dossier n'existe pas, la fonction le crée. Pour chaque frame, la fonction génère un nom d'image unique en fonction du préfixe et de l'indice de la frame, puis enregistre l'image convertie en format BGR dans le dossier. Ainsi, cette fonction facilite le stockage des frames extraites dans un emplacement spécifié sur le système de fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames_to_folder(frames, folder_path, image_prefix=\"frame\"):\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Save each frame to the folder\n",
    "    for i, frame in enumerate(frames):\n",
    "        image_name = f\"{image_prefix}_{i + 1}.png\"\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "        cv2.imwrite(image_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Détection des yeux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `eye_aspect_ratio` calcule le Rapport d'Aspect des Yeux (EAR) à partir des coordonnées des points caractéristiques du contour des yeux. Elle prend en entrée une liste de tuples représentant les coordonnées des points caractéristiques du contour des yeux, généralement obtenue à partir de la détection de visage et de la prédiction des points de repère. La formule pour calculer l'EAR est définie comme suit :\n",
    "\n",
    "$$\n",
    "\\text{EAR} = \\frac{(\\text{p2} - \\text{p6}) + (\\text{p3} - \\text{p5})}{2 \\times (\\text{p1} - \\text{p4})}\n",
    "$$\n",
    "\n",
    "où p1, p2, p3, p4, p5 et p6 représentent des points caractéristiques spécifiques du contour des yeux, et $\\text{dist.euclidean}$ est une fonction qui calcule la distance euclidienne entre deux points. Le résultat de cette fonction est l'EAR, souvent utilisé dans la détection de la somnolence en surveillant les changements de forme des yeux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "    p2_minus_p6 = dist.euclidean(eye[1], eye[5])\n",
    "    p3_minus_p5 = dist.euclidean(eye[2], eye[4])\n",
    "    p1_minus_p4 = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (p2_minus_p6 + p3_minus_p5) / (2.0 * p1_minus_p4)\n",
    "    return ear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **FACIAL_LANDMARK_PREDICTOR :** Chemin vers le prédicteur de points caractéristiques faciaux pré-entraîné de dlib.\n",
    "\n",
    "* **MINIMUM_EAR :** Valeur minimale de l'EAR au-dessus de laquelle les yeux seront considérés comme ouverts, sinon fermés. Ce paramètre peut nécessiter un ajustement en fonction de vos besoins. Essayez de déterminer l'EAR dans différents scénarios, puis ajustez la valeur en conséquence. Notez également que cet EAR n'est pas pour un seul œil, mais l'EAR cumulé pour les deux yeux.\n",
    "\n",
    "* **MAXIMUM_FRAME_COUNT :** La valeur de l'EAR change très rapidement. Même si vous clignez des yeux, l'EAR diminuera rapidement. Cependant, cligner des yeux ne signifie pas nécessairement la somnolence. La somnolence serait une situation où une personne a fermé les yeux (son EAR est très bas) pendant, disons, 10 images vidéo consécutives. Cette variable indique le nombre maximum d'images vidéo consécutives pendant lesquelles l'EAR peut rester inférieur à MINIMUM_EAR, sinon une alerte de somnolence est déclenchée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACIAL_LANDMARK_PREDICTOR = \"shape_predictor_68_face_landmarks.dat\"  \n",
    "MINIMUM_EAR = 0.2\n",
    "MAXIMUM_FRAME_COUNT = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instancier le détecteur de visage faceDetector de dlib (qui détectera le visage dans une image) et le chercheur de points de repère landmarkFinder (qui trouvera 68 points de repère dans le visage détecté)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceDetector = dlib.get_frontal_face_detector()\n",
    "landmarkFinder = dlib.shape_predictor(FACIAL_LANDMARK_PREDICTOR)\n",
    "landmarkPredictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "webcamFeed = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nous trouvons les valeurs de début et de fin des identifiants de points de repère pour chaque œil. Vous pouvez le faire manuellement (37-42 pour l'œil droit et 43-48 pour l'œil gauche), mais en utilisant face_utils, vous pouvez obtenir ces valeurs en passant simplement le nom de l'œil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "(leftEyeStart, leftEyeEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rightEyeStart, rightEyeEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_eyes_ml(input_folder, output_folder):\n",
    "    # Vérifier si le dossier de sortie existe, sinon le créer\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Initialiser le détecteur de visages de dlib\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    \n",
    "    # Initialiser le prédicteur de points de visage pour détecter les yeux\n",
    "    eye_predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "    # Liste des fichiers dans le dossier d'entrée\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    for image_file in image_files:\n",
    "        # Chemin complet de l'image d'entrée\n",
    "        input_path = os.path.join(input_folder, image_file)\n",
    "\n",
    "        # Lire l'image\n",
    "        image = cv2.imread(input_path)\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Détecter les visages dans l'image\n",
    "        faces = face_detector(gray_image)\n",
    "\n",
    "        # Dessiner des rectangles autour des yeux détectés\n",
    "        image_with_eyes = image.copy()\n",
    "        for face in faces:\n",
    "            landmarks = eye_predictor(gray_image, face)\n",
    "            for n in range(36, 48):  # Les indices 36 à 47 représentent les yeux dans les landmarks\n",
    "                x, y = landmarks.part(n).x, landmarks.part(n).y\n",
    "                cv2.circle(image_with_eyes, (x, y), 2, (0, 255, 0), -1)\n",
    "\n",
    "        # Chemin complet de l'image de sortie\n",
    "        output_path = os.path.join(output_folder, f\"{os.path.splitext(image_file)[0]}_eyes_detected_ml.jpg\")\n",
    "\n",
    "        # Enregistrer l'image avec les yeux détectés\n",
    "        cv2.imwrite(output_path, image_with_eyes)\n",
    "        print(f\"Yeux détectés avec modèle ML enregistrés : {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Application de techniques de morphologie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erosion(frames, kernel_size=3, iterations=1):\n",
    "    # Define the kernel\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    \n",
    "    # Apply erosion to each frame\n",
    "    result_frames = [cv2.erode(frame, kernel, iterations=iterations) for frame in frames]\n",
    "    \n",
    "    return result_frames\n",
    "\n",
    "def dilation(frames, kernel_size=3, iterations=1):\n",
    "    # Define the kernel\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    \n",
    "    # Apply dilation to each frame\n",
    "    result_frames = [cv2.dilate(frame, kernel, iterations=iterations) for frame in frames]\n",
    "    \n",
    "    return result_frames\n",
    "\n",
    "def opening(frames, kernel_size=3, iterations=1):\n",
    "    # Define the kernel\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    \n",
    "    # Apply opening (erosion followed by dilation) to each frame\n",
    "    eroded_frames = [cv2.erode(frame, kernel, iterations=iterations) for frame in frames]\n",
    "    result_frames = [cv2.dilate(eroded, kernel, iterations=iterations) for eroded in eroded_frames]\n",
    "    \n",
    "    return result_frames\n",
    "\n",
    "def closing(frames, kernel_size=3, iterations=1):\n",
    "    # Define the kernel\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    \n",
    "    # Apply closing (dilation followed by erosion) to each frame\n",
    "    dilated_frames = [cv2.dilate(frame, kernel, iterations=iterations) for frame in frames]\n",
    "    result_frames = [cv2.erode(dilated, kernel, iterations=iterations) for dilated in dilated_frames]\n",
    "    \n",
    "    return result_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Récupération des points d'intérêts (p1,...p6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "    # Convert the list of tuples to NumPy array\n",
    "    eye = np.array(eye)\n",
    "    \n",
    "    # Vertical distances between eye landmarks\n",
    "    v1 = np.linalg.norm(eye[1] - eye[5])\n",
    "    v2 = np.linalg.norm(eye[2] - eye[4])\n",
    "    \n",
    "    # Horizontal distance between the outer eye corners\n",
    "    h = np.linalg.norm(eye[0] - eye[3])\n",
    "    \n",
    "    # Eye aspect ratio\n",
    "    ear = (v1 + v2) / (2 * h)\n",
    "    return ear\n",
    "\n",
    "def detect_eyes_and_save_images_and_ear(input_folder, output_folder):\n",
    "    # Check if the output folder exists, create it if not\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Initialize the dlib face detector\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    # Initialize the dlib shape predictor for eye detection\n",
    "    eye_predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "    # Create a CSV file to save EAR values\n",
    "    csv_file_path = os.path.join(output_folder, \"ear_values.csv\")\n",
    "    with open(csv_file_path, mode='w', newline='') as csvfile:\n",
    "        fieldnames = ['Image', 'EAR']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Liste des fichiers dans le dossier d'entrée\n",
    "        image_files = [f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        for image_file in image_files:\n",
    "            # Chemin complet de l'image d'entrée\n",
    "            input_path = os.path.join(input_folder, image_file)\n",
    "\n",
    "            # Lire l'image\n",
    "            image = cv2.imread(input_path)\n",
    "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Détecter les visages dans l'image\n",
    "            faces = face_detector(gray_image)\n",
    "\n",
    "            # Vérifier si des visages sont détectés\n",
    "            if faces:\n",
    "                # Créer une copie de l'image pour dessiner les cercles sans affecter l'originale\n",
    "                image_with_eyes = image.copy()\n",
    "\n",
    "                # Dessiner des cercles autour des yeux sur la copie de l'image\n",
    "                for face in faces:\n",
    "                    landmarks = eye_predictor(gray_image, face)\n",
    "                    for n in range(36, 48):  # Les indices 36 à 47 représentent les yeux dans les landmarks\n",
    "                        x, y = landmarks.part(n).x, landmarks.part(n).y\n",
    "                        cv2.circle(image_with_eyes, (x, y), 2, (0, 255, 0), -1)\n",
    "\n",
    "                # Chemin complet de l'image de sortie avec les yeux détectés\n",
    "                output_path_with_eyes = os.path.join(output_folder, f\"{os.path.splitext(image_file)[0]}_with_eyes.jpg\")\n",
    "                cv2.imwrite(output_path_with_eyes, image_with_eyes)\n",
    "                print(f\"Image avec yeux détectés enregistrée : {output_path_with_eyes}\")\n",
    "\n",
    "                # Calculer et enregistrer l'EAR pour chaque œil dans l'image\n",
    "                for face in faces:\n",
    "                    landmarks = eye_predictor(gray_image, face)\n",
    "                    left_eye = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(36, 42)]\n",
    "                    right_eye = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(42, 48)]\n",
    "                    \n",
    "                    # Calculer l'EAR pour les yeux gauche et droit\n",
    "                    ear_left = eye_aspect_ratio(left_eye)\n",
    "                    ear_right = eye_aspect_ratio(right_eye)\n",
    "                    \n",
    "                    # EAR moyen des deux yeux\n",
    "                    ear_avg = (ear_left + ear_right) / 2\n",
    "\n",
    "                    # Enregistrer la valeur de l'EAR dans le fichier CSV\n",
    "                    writer.writerow({'Image': image_file, 'EAR': ear_avg})\n",
    "\n",
    "                    print(f\"EAR calculé pour {image_file}: {ear_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test de l'application sur une vidéo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = \"videos/video-1.mp4\"\n",
    "# t = 10 # Extract frames every 10 seconds\n",
    "\n",
    "# img_bruit = ajouter_bruit_impulsionnel(img, 0.1)\n",
    "# frames = extract_frames_at_intervals(video_path, interval=t)\n",
    "# save_frames_to_folder(frames, \"input_frames\", image_prefix=\"frame\")\n",
    "# gray_frames = convert_to_grayscale(frames)\n",
    "\n",
    "# # Afficher les images extraites\n",
    "# plot_images(frames, [f\"Frame extract {i+1}\" for i in range(len(frames))])\n",
    "\n",
    "# # Afficher les images en niveaux de gris\n",
    "# for i, image_gray in enumerate(gray_frames):\n",
    "#     # Convert grayscale image to BGR for correct display\n",
    "#     image_bgr = cv2.cvtColor(image_gray, cv2.COLOR_GRAY2BGR)\n",
    "#     gray_frames[i] = image_bgr\n",
    "    \n",
    "# # plot_images(gray_frames, [f\"Gray Frame {i+1}\" for i in range(len(gray_frames))])\n",
    "\n",
    "# # Supprimer le bruit des images\n",
    "# frames_without_noise = remove_noise(frames, method='gaussian', kernel_size=5, sigma=1.0)\n",
    "# plot_images(frames_without_noise, [f\"Frame without noise {i+1}\" for i in range(len(frames_without_noise))])\n",
    "\n",
    "# # Améliorer la qualité des images\n",
    "# enhanced_frames = enhance_images(frames_without_noise)\n",
    "# plot_images(enhanced_frames, [f\"Enhanced Frame {i+1}\" for i in range(len(enhanced_frames))])\n",
    "\n",
    "# # Détecter les bords dans les images\n",
    "# edges_frames = detect_edges(enhanced_frames)\n",
    "# plot_images(edges_frames, [f\"Edges Frame {i+1}\" for i in range(len(edges_frames))])\n",
    "\n",
    "# # Les opérations morphologiques\n",
    "# output_folder = \"morphologies_images\"\n",
    "\n",
    "# # Erosion\n",
    "# eroded_frames = erosion(enhanced_frames, kernel_size=3, iterations=1)\n",
    "# plot_images(eroded_frames, [f\"Eroded Frame {i+1}\" for i in range(len(eroded_frames))])\n",
    "# save_frames_to_folder(frames=eroded_frames, folder_path=output_folder)\n",
    "\n",
    "# # Dilation\n",
    "# dilated_frames = dilation(enhanced_frames, kernel_size=3, iterations=1)\n",
    "# plot_images(dilated_frames, [f\"Dilated Frame {i+1}\" for i in range(len(dilated_frames))])\n",
    "# save_frames_to_folder(frames=dilated_frames, folder_path=output_folder)\n",
    "\n",
    "# # Opening\n",
    "# opened_frames = opening(enhanced_frames, kernel_size=3, iterations=1)\n",
    "# save_frames_to_folder(frames=opened_frames, folder_path=output_folder)\n",
    "\n",
    "# # Enregistrer les images dans un dossier\n",
    "# output_folder = \"output_frames\"\n",
    "# save_frames_to_folder(edges_frames, output_folder, image_prefix=\"frame_edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeux détectés avec modèle ML enregistrés : eyes_detected/frame_1_eyes_detected_ml.jpg\n",
      "Yeux détectés avec modèle ML enregistrés : eyes_detected/frame_3_eyes_detected_ml.jpg\n",
      "Yeux détectés avec modèle ML enregistrés : eyes_detected/frame_2_eyes_detected_ml.jpg\n"
     ]
    }
   ],
   "source": [
    "input_folder = \"input_frames\"\n",
    "output_folder = \"eyes_detected\"\n",
    "\n",
    "detect_eyes_ml(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image avec yeux détectés enregistrée : ear_results/frame_1_with_eyes.jpg\n",
      "EAR calculé pour frame_1.png: 0.46194877289478753\n",
      "EAR calculé pour frame_1.png: 0.4589423674778521\n",
      "EAR calculé pour frame_1.png: 0.3491574212460923\n",
      "Image avec yeux détectés enregistrée : ear_results/frame_3_with_eyes.jpg\n",
      "EAR calculé pour frame_3.png: 0.4120040431297233\n",
      "EAR calculé pour frame_3.png: 0.4250110042739885\n",
      "EAR calculé pour frame_3.png: 0.4218599905051843\n",
      "Image avec yeux détectés enregistrée : ear_results/frame_2_with_eyes.jpg\n",
      "EAR calculé pour frame_2.png: 0.4586299078608632\n",
      "EAR calculé pour frame_2.png: 0.38387322171731875\n",
      "EAR calculé pour frame_2.png: 0.4332454907937815\n"
     ]
    }
   ],
   "source": [
    "input_folder = \"morphologies_images\"\n",
    "output_csv = \"ear_results\"\n",
    "\n",
    "detect_eyes_and_save_images_and_ear(input_folder, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize frame counter\n",
    "EYE_CLOSED_COUNTER = 0\n",
    "\n",
    "# Set the window name\n",
    "cv2.namedWindow(\"Frame\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Set the desired window size\n",
    "cv2.resizeWindow(\"Frame\", 600, 600)  # Adjust the size as needed\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Read the frame from the webcam feed\n",
    "        (status, image) = webcamFeed.read()\n",
    "        image = imutils.resize(image, width=800)\n",
    "        grayImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the grayscale image\n",
    "        faces = faceDetector(grayImage, 0)\n",
    "\n",
    "        for face in faces:\n",
    "            # Predict facial landmarks\n",
    "            faceLandmarks = landmarkPredictor(grayImage, face)\n",
    "            faceLandmarks = face_utils.shape_to_np(faceLandmarks)\n",
    "\n",
    "            # Extract left and right eyes\n",
    "            leftEye = faceLandmarks[leftEyeStart:leftEyeEnd]\n",
    "            rightEye = faceLandmarks[rightEyeStart:rightEyeEnd]\n",
    "\n",
    "            # Calculate eye aspect ratio (EAR)\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            leftEyeHull = cv2.convexHull(leftEye)\n",
    "            rightEyeHull = cv2.convexHull(rightEye)\n",
    "            cv2.drawContours(image, [leftEyeHull], -1, (255, 0, 0), 2)\n",
    "            cv2.drawContours(image, [rightEyeHull], -1, (255, 0, 0), 2)\n",
    "\n",
    "            # Check for drowsiness\n",
    "            if ear < MINIMUM_EAR:\n",
    "                EYE_CLOSED_COUNTER += 1\n",
    "            else:\n",
    "                EYE_CLOSED_COUNTER = 0\n",
    "\n",
    "            # Display EAR value on the frame\n",
    "            cv2.putText(image, f\"EAR: {round(ear, 2)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            # Check if drowsiness is detected\n",
    "            if EYE_CLOSED_COUNTER >= MAXIMUM_FRAME_COUNT:\n",
    "                cv2.putText(image, \"Drowsiness\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "                # Play alert sound (system beep)\n",
    "                if platform.system() == \"Windows\":\n",
    "                    os.system(\"rundll32.exe user32.dll,MessageBeep -1\")\n",
    "                else:\n",
    "                    pass\n",
    "        \n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Frame\", image)\n",
    "        \n",
    "        # Wait for a key press\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Release the webcam feed and close all windows\n",
    "webcamFeed.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (657700636.py, line 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[188], line 62\u001b[0;36m\u001b[0m\n\u001b[0;31m    os.\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Initialize frame counter\n",
    "EYE_CLOSED_COUNTER = 0\n",
    "video_path = \"videos/video-2.mp4\"\n",
    "webcamFeed = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Specify the desired width for the displayed frame\n",
    "desired_width = 360\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Read the frame from the webcam feed\n",
    "        (status, image) = webcamFeed.read()\n",
    "\n",
    "        if not status:\n",
    "            break  # Break the loop if the video has ended\n",
    "\n",
    "        # Resize the frame to the desired width\n",
    "        image = imutils.resize(image, width=desired_width)\n",
    "\n",
    "        grayImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the grayscale image\n",
    "        faces = faceDetector(grayImage, 0)\n",
    "\n",
    "        for face in faces:\n",
    "            # Predict facial landmarks\n",
    "            faceLandmarks = landmarkPredictor(grayImage, face)\n",
    "            faceLandmarks = face_utils.shape_to_np(faceLandmarks)\n",
    "\n",
    "            # Extract left and right eyes\n",
    "            leftEye = faceLandmarks[leftEyeStart:leftEyeEnd]\n",
    "            rightEye = faceLandmarks[rightEyeStart:rightEyeEnd]\n",
    "\n",
    "            # Calculate eye aspect ratio (EAR)\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            leftEyeHull = cv2.convexHull(leftEye)\n",
    "            rightEyeHull = cv2.convexHull(rightEye)\n",
    "            cv2.drawContours(image, [leftEyeHull], -1, (255, 0, 0), 2)\n",
    "            cv2.drawContours(image, [rightEyeHull], -1, (255, 0, 0), 2)\n",
    "\n",
    "            # Check for drowsiness\n",
    "            if ear < MINIMUM_EAR:\n",
    "                EYE_CLOSED_COUNTER += 1\n",
    "            else:\n",
    "                EYE_CLOSED_COUNTER = 0\n",
    "\n",
    "            # Display EAR value on the frame\n",
    "            cv2.putText(image, f\"EAR: {round(ear, 2)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            # Check if drowsiness is detected\n",
    "            if EYE_CLOSED_COUNTER >= MAXIMUM_FRAME_COUNT:\n",
    "                cv2.putText(image, \"Drowsiness\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "                # Play alert sound (system beep)\n",
    "                if platform.system() == \"Windows\":\n",
    "                    os.system(\"rundll32.exe user32.dll,MessageBeep -1\")\n",
    "                else:\n",
    "                    pass\n",
    "        \n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Frame\", image)\n",
    "\n",
    "        # Wait for a key press\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Release the webcam feed and close all windows\n",
    "webcamFeed.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "akamenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
